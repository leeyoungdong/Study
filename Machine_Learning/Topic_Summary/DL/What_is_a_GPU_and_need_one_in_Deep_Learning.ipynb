{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFcd7VEfwMPy"
      },
      "source": [
        "# **딥러닝할 때 GPU를 쓰는 이유**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plmz2L-mwOAw"
      },
      "source": [
        "- GPU(Graphics Processing Unit)은 부동 소수점 연산을 수행하는 많은 코어가 있어 수 많은 연산을 병렬처리할 수 있다.\n",
        "- CPU보다 더 큰 메모리 대역폭을 가지고 있기 때문에 큰 데이터를 더 효율적으로 빠르게 처리할 수 있다.\n",
        "\n",
        "**메모리 대역폭(Memory Bandwidth)란 메모리가 처리할 수 있는 초당 데이터양을 뜻한다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pytorch의 경우 torch.nn.DataParallel을 사용하여 여러 개의 GPU를 사용할 수 있다.**\n",
        "- torch.device를 cuda로 설정.\n",
        "- nn.DataParallel을 사용하여 모델을 감싼다.\n",
        "- 모델을 model.to(device)를 사용하여 GPU로 보낸다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **학습시 필요한 GPU 메모리 계산**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Pytorch를 기준으로 볼 때 something.to('cuda')로 변환하는 모든 것들을 생각해보면 된다.\n",
        "- GPU로 올리는 것은 모델과 데이터셋이므로, (모델의 크기 + 데이터의 크기 × 배치 크기)로 학습시 필요한 메모리 크기를 계산."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
