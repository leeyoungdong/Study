{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **스파크 colab에 환경 구현**"
      ],
      "metadata": {
        "id": "a_xbGA760V0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HwEL73ilw7oe"
      },
      "outputs": [],
      "source": [
        "# jdk 툴 설치\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# apache spark download - 하둡 버전에 따라 spakr 와 하둡 변경 (밑에 tgz 파일 설치 및 pip install)\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "# 명령어로 spark - hadoop 파일 압축 풀기\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "# spark 설치\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# spark 환경변수 설정 - java(jdk) ,spark(spark) 2개다 설정해줘야함\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wuSEjqETxB5E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import spark 말고 findspark로 spark파일을 못찾는경우에 빠르게 설치 가능\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tMxcs5cSxXAN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "id": "ovIHdzfFymu7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "2c8c58a5-da54-40ac-f8b3-a108f90e6f94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f4caa1fee90>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://aaf4af1826f2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **연습창 (spark)**"
      ],
      "metadata": {
        "id": "OqVkPKvx1KbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "tabKdtPR3VZe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tichers = sc.parallelize({\n",
        "    (1, ('Google', 'GOOGL', 'USA')),\n",
        "    (2, ('Netflix', 'NFLX', 'USA')),\n",
        "    (3, ('Amazon', 'AMZN', 'USA')),\n",
        "    (4, ('Tesla', 'TSLA', 'USA')),\n",
        "    (5, ('Samsung', '005930', 'Korea')),\n",
        "    (6, ('KaKao', '035720', 'Korea'))\n",
        "})\n",
        "\n",
        "prices = sc.parallelize([\n",
        "    (1, (2984, 'USD')),\n",
        "    (2, (645, 'USD')),\n",
        "    (3, (3518, 'USD')),\n",
        "    (4, (1222, 'USD')),\n",
        "    (5, (70600, 'USD')),\n",
        "    (6, (125000, 'USD')),\n",
        "])"
      ],
      "metadata": {
        "id": "u0z3x9NXxT9B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "만약 위 코드에서 미국의 2000불 이상의 주식만 가져오기 위한 방법은 3가지로 생각해볼 수 있다.\n",
        "\n",
        "- Inner Join\n",
        " \n",
        "- Filter by Coutry\n",
        "\n",
        "- Filter by Currency\n"
      ],
      "metadata": {
        "id": "KDYdRrlecD4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CASE 1: join 먼저, filter 나중에\n",
        "tickerPrice = tichers.join(prices)\n",
        "tickerPrice.filter(lambda x: x[1][0][2] == 'USA' and x[1][1][0] > 2000).collect()\n",
        "'''\n",
        "[(1, (('Google', 'GOOGL', 'USA'), (2984, 'USD'))), 3, (('Amazon', 'AMZN', 'USA'), (3518, 'USD')))]\n",
        "'''\n",
        "\n",
        "# CASE 2: filter 먼저, join 나중에\n",
        "filteredTicker = tichers.filter(lambda x: x[1][2] == 'USA')\n",
        "filteredTicker = prices.filter(lambda x: x[1][0] > 2000)\n",
        "filteredTicker.join(tickerPrice).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDMICAJ9cDvO",
        "outputId": "ab278c8e-1b9e-45e0-c396-3bcba01eaf92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(6, ((125000, 'USD'), (('KaKao', '035720', 'Korea'), (125000, 'USD')))),\n",
              " (1, ((2984, 'USD'), (('Google', 'GOOGL', 'USA'), (2984, 'USD')))),\n",
              " (3, ((3518, 'USD'), (('Amazon', 'AMZN', 'USA'), (3518, 'USD')))),\n",
              " (5, ((70600, 'USD'), (('Samsung', '005930', 'Korea'), (70600, 'USD'))))]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 가지의 경우 같은 결과를 낳지만 퍼포먼스 자체는 두 번째 케이스가 좋다.\n",
        "\n",
        "연산에 대하여 일일이 신경쓰기란 까다롭다.\n",
        "\n",
        "네트워크 연산 성능에 대하여 만약 데이터가 구조화되어 있다면 자동으로 최적화가 가능하다\n",
        ".\n",
        "\n",
        "구조화된 데이터란 정형, 비정형, 반정형데이터를 뜻한다."
      ],
      "metadata": {
        "id": "Q39HvXp1cMW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정형(Structured), 비정형(Unstructured), 반정형(Semi structured)**\n"
      ],
      "metadata": {
        "id": "ztbv5YkrcRVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정형: 행과 열이 있고 데이터 타입이 스키마인 데이터이다.\n",
        "\n",
        "데이터 베이스\n",
        "\n",
        "비정형: 자유 형식으로 정리가 되지 않은 파일이다.\n",
        "\n",
        "로그 파일\n",
        "이미지\n",
        "\n",
        "반정형: 행과 열이 있는 데이터이다.\n",
        "\n",
        "CSV\n",
        "JSON\n",
        "XML"
      ],
      "metadata": {
        "id": "PkCroommcUjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정형 데이터와 RDD**\n",
        "\n",
        "RDD에서는 데이터의 구조를 모르기 때문에 데이터를 다루는 것을 개발자에게 의존할 수 밖에 없다.\n",
        "\n",
        "map, flatMap, filter 등을 통해 유저가 만든 함수를 수행한다.\n",
        "\n",
        "하지만 정형 데이터에서는 데이터의 구조를 이미 알고 있으므로 어떤 테스크를 수행할 것인지 정의만 하면 된다.\n",
        "\n",
        "최적화도 자동으로 진행된다."
      ],
      "metadata": {
        "id": "-DQmthlU5sdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL\n",
        "\n",
        "Spark SQL은 구조화된 데이터를 다룰 수 있게 해준다.\n",
        "\n",
        "유저가 일일이 함수를 정의하는 일 없이 작업을 수행할 수 있고 자동으로 연산이 최적화된다."
      ],
      "metadata": {
        "id": "GKzfAYfdu75k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **스파크(Spark) SQL**\n",
        "# 목적\n",
        "스파크 프로그래밍 내부에서 관계형 처리를 할 수 있다.\n",
        "\n",
        "스키마 정보를 이용해 자동으로 최적화를 할 수 있다.\n",
        "\n",
        "외부 데이터셋을 쉽게 사용할 수 있다.\n",
        "# 소개\n",
        "스파크 위에 구현된 하나의 패키지이다.\n",
        "\n",
        "3개의 주요 API가 존재한다.\n",
        "\n",
        "SQL\n",
        "\n",
        "DataFrame\n",
        "\n",
        "Datasets\n",
        "\n",
        "2개의 백엔드 컴포넌트로 최적화를 진행한다.\n",
        "\n",
        "Catalyst: 쿼리 최적화 엔진\n",
        "\n",
        "Tungsten: 시리얼라이저(용량)\n",
        "# 데이터 프레임(DataFrame)\n",
        "스파크 코어(Core)에 RDD가 있다면 스파크 SQL에는 데이터 프레임이 있다.\n",
        "\n",
        "데이터 프레임은 테이블 데이터셋이다.\n",
        "\n",
        "개념적으로는 RDD에 스키마가 적용된 것이라 볼 수 있다.\n",
        "\n",
        "**데이터 프레임 생성**\n",
        "\n",
        "RDD에서 스키마를 정의한 다음 변형 하는 방법과 CSV, JSON 등의 데이터를 받아오는 방법이 있다.\n",
        "\n",
        "**RDD로 데이터 프레임 생성**\n",
        "\n",
        "스키마를 자동으로 유추하여 데이터 프레임을 만들거나, 스키마를 사용자가 정의하는 방법이 있다."
      ],
      "metadata": {
        "id": "jaXOFZH8vD-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**createOrReplaceTempView**\n",
        "\n",
        "데이터 프레임을 하나의 데이터 베이스 테이블 처럼 사용하려면 createOrReplaceTempView()함수로 temporary view를 만들어줘야한다"
      ],
      "metadata": {
        "id": "zHqrU8p3vy9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.creatOrReplaceTempView('mobility_data') # 닉네임 지정\n",
        "spark.sql('SELECT pickup_datetime FROM mobility_data LIMIT 5').show()\n"
      ],
      "metadata": {
        "id": "EMcDQIa9vDyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **스파크 세션(SparkSession)**\n",
        "\n",
        "스파크 코어에 스파크 컨텍스트가 있었다면 스파크 SQL엔 스파크 세션이 있다.\n",
        "\n",
        "파이썬에서 스파크 SQL을 사용하기 위한 방법이며 스파크 세션으로 불러오는 데이터는 데이터 프레임이다."
      ],
      "metadata": {
        "id": "ju8m3JCVv78E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"test-app\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "L6kqtOyhwA27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위와 같은 코드로 스파크 세션을 만들어 줄 것이다.\n",
        "\n",
        "SQL문 뿐만 아니라 함수를 사용해서도 가능하다.\n",
        "\n",
        "데이터 프레임을 RDD로 변환하여 사용할 수도 있지만(rdd = df.rdd.map(tuple)), RDD를 덜 사용하는 쪽이 좋다."
      ],
      "metadata": {
        "id": "qNZtZfoZwBhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스파크에서 사용할 수 있는 SQL문**\n",
        "\n",
        "하이브 쿼리 언어(Hive Query Language)와 거의 동일하다.\n",
        "\n",
        "Select\n",
        "\n",
        "From\n",
        "\n",
        "Where\n",
        "\n",
        "Count\n",
        "\n",
        "Having\n",
        "\n",
        "Group By\n",
        "\n",
        "Order By\n",
        "\n",
        "Sort By\n",
        "\n",
        "Distinct\n",
        "\n",
        "Join\n",
        "\n",
        "**데이터 프레임의 이점**\n",
        "\n",
        "위에서 RDD를 덜 사용하는 편이 좋다고 했는데, 그 이유는 MLlib이나 스파크 스트리밍(Spark Streaming)과 같은 다른 스파크 모듈과 사용하기엔 데이터 프레임이 좋기 때문이다.\n",
        "\n",
        "개발하기에도 편하고 최적화도 알아서 된다.\n",
        "\n",
        "**데이터셋(Datasets)**\n",
        "\n",
        "타입이 있는 데이터프레임이며 파이스파크에선 크게 신경쓰지 않아도 된다."
      ],
      "metadata": {
        "id": "aCywqsqpwGFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# 스파크 세션 생성\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"learn-sql\").getOrCreate()\n",
        "\n",
        "\n",
        "# 주식 데이터 생성\n",
        "stocks = [\n",
        "    ('Google', 'GOOGL', 'USA', 2984, 'USD'), \n",
        "    ('Netflix', 'NFLX', 'USA', 645, 'USD'),\n",
        "    ('Amazon', 'AMZN', 'USA', 3518, 'USD'),\n",
        "    ('Tesla', 'TSLA', 'USA', 1222, 'USD'),\n",
        "    ('Tencent', '0700', 'Hong Kong', 483, 'HKD'),\n",
        "    ('Toyota', '7203', 'Japan', 2006, 'JPY'),\n",
        "    ('Samsung', '005930', 'Korea', 70600, 'KRW'),\n",
        "    ('Kakao', '035720', 'Korea', 125000, 'KRW'),\n",
        "]\n"
      ],
      "metadata": {
        "id": "60KURJ_rxT7H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스키마 생성\n",
        "# 컬럼의 이름만 입력하고 데이터 타입은 정하지 않는다.\n",
        "stockSchema = [\"name\", \"ticker\", \"country\", \"price\", \"currency\"]\n",
        "\n",
        "\n",
        "# 데이터 프레임 생성\n",
        "df = spark.createDataFrame(data=stocks, schema=stockSchema)\n",
        "\n",
        "\n",
        "# 데이터 타입 확인\n",
        "df.dtypes\n",
        "'''\n",
        "[('name', 'string'),\n",
        " ('ticker', 'string'),\n",
        " ('country', 'string'),\n",
        " ('price', 'bigint'),\n",
        " ('currency', 'string')]\n",
        "'''\n",
        "\n",
        "\n",
        "# 데이터 프레임 확인\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br4eKTBCKVod",
        "outputId": "91f6087a-61d7-41b8-98f5-6a75066e9b1b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------+------+--------+\n",
            "|   name|ticker|  country| price|currency|\n",
            "+-------+------+---------+------+--------+\n",
            "| Google| GOOGL|      USA|  2984|     USD|\n",
            "|Netflix|  NFLX|      USA|   645|     USD|\n",
            "| Amazon|  AMZN|      USA|  3518|     USD|\n",
            "|  Tesla|  TSLA|      USA|  1222|     USD|\n",
            "|Tencent|  0700|Hong Kong|   483|     HKD|\n",
            "| Toyota|  7203|    Japan|  2006|     JPY|\n",
            "|Samsung|005930|    Korea| 70600|     KRW|\n",
            "|  Kakao|035720|    Korea|125000|     KRW|\n",
            "+-------+------+---------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TempView에 등록을 하여야 사용할 수 있다.\n",
        "df.createOrReplaceTempView(\"stocks\")"
      ],
      "metadata": {
        "id": "5vWrDm57Ke8b"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name FROM stocks\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLT6XkAiKjVb",
        "outputId": "002ad8ce-9256-4ee4-8447-66f891d8469e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "| Google|\n",
            "|Netflix|\n",
            "| Amazon|\n",
            "|  Tesla|\n",
            "|Tencent|\n",
            "| Toyota|\n",
            "|Samsung|\n",
            "|  Kakao|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, price FROM stocks\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7e_7HgPspB0",
        "outputId": "a8ee5d3e-b6f2-4e15-ceea-ea5f4f02878c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   name| price|\n",
            "+-------+------+\n",
            "| Google|  2984|\n",
            "|Netflix|   645|\n",
            "| Amazon|  3518|\n",
            "|  Tesla|  1222|\n",
            "|Tencent|   483|\n",
            "| Toyota|  2006|\n",
            "|Samsung| 70600|\n",
            "|  Kakao|125000|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, price FROM stocks WHERE country = 'Korea'\").show(), spark.sql(\"SELECT name, price FROM stocks WHERE price > 2000\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GXx5uYCsru0",
        "outputId": "705bf14c-0f16-4b9c-ccfb-56259df78c1d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   name| price|\n",
            "+-------+------+\n",
            "|Samsung| 70600|\n",
            "|  Kakao|125000|\n",
            "+-------+------+\n",
            "\n",
            "+-------+------+\n",
            "|   name| price|\n",
            "+-------+------+\n",
            "| Google|  2984|\n",
            "| Amazon|  3518|\n",
            "| Toyota|  2006|\n",
            "|Samsung| 70600|\n",
            "|  Kakao|125000|\n",
            "+-------+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, price FROM stocks WHERE price > 2000 and country = 'USA'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWOz5-7qswJ7",
        "outputId": "f0fba78b-0e13-4a41-c1a7-6cb248162b70"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  name|price|\n",
            "+------+-----+\n",
            "|Google| 2984|\n",
            "|Amazon| 3518|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, price FROM stocks WHERE country LIKE 'U%'\").show() # U로 시작"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bg4LtmWsyX2",
        "outputId": "0674c0ba-44ab-476d-eb9a-7e2d71600987"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   name|price|\n",
            "+-------+-----+\n",
            "| Google| 2984|\n",
            "|Netflix|  645|\n",
            "| Amazon| 3518|\n",
            "|  Tesla| 1222|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT name, price, currency FROM stocks \\\n",
        "WHERE currency = 'USD' AND \\\n",
        "price > (SELECT price FROM stocks WHERE NAME = 'Tesla')\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ_--zkms0Pz",
        "outputId": "0abf8d73-3805-45e7-e1e8-c27673916416"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+--------+\n",
            "|  name|price|currency|\n",
            "+------+-----+--------+\n",
            "|Google| 2984|     USD|\n",
            "|Amazon| 3518|     USD|\n",
            "+------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT sum(price) FROM stocks WHERE country = 'Korea'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXzNPXths3wz",
        "outputId": "8e1696b1-fe81-4ac4-b990-32cc6ba027f6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(price)|\n",
            "+----------+\n",
            "|    195600|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT mean(price) FROM stocks WHERE country = 'Korea'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzsvBmmss5SL",
        "outputId": "54d1cdaf-b498-47b0-8daf-aa0bb8cc0460"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|mean(price)|\n",
            "+-----------+\n",
            "|    97800.0|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터 프레임(DataFrame)**\n",
        "데이터 프레임은 관계형 데이터셋(RDD + Relation)이다.\n",
        "\n",
        "RDD가 함수형 API를 가졌다면 데이터 프레임은 선언형 API이다.\n",
        "\n",
        "스키마를 가졌기 때문에 자동으로 최적화가 가능하다.\n",
        "\n",
        "타입이 없다.(데이터 프레임 내부적으로 타입을 관제하지 않는다.)\n",
        "\n",
        "# **데이터 프레임의 특징**\n",
        "데이터 프레임은 RDD의 확장판이다.\n",
        "\n",
        "RDD와 같이 지연 실행(Lazy Execution)된다.\n",
        "분산 저장된다.\n",
        "\n",
        "불변(immutabel) 데이터이다.\n",
        "\n",
        "열(row) 객체가 있다.\n",
        "\n",
        "SQL 쿼리를 직접 바로 실행할 수 있다.\n",
        "\n",
        "스키마를 가질 수 있고, 이를 통해 성능을 더욱 최적화 할 수 있다.\n",
        "\n",
        "CSV, JSON, Hive 등으로 읽어오거나 변환이 가능하다.\n"
      ],
      "metadata": {
        "id": "Jrd_nINDwbyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터 프레임의 스키마 확인**\n",
        "**dtypes**\n",
        "내부 스키마를 볼 수 있다."
      ],
      "metadata": {
        "id": "T4sYWl0myCDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omsD4z5-yFts",
        "outputId": "90c1b30c-cb8d-4ee9-ca24-5da63f0deeb6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_1', 'bigint'), ('_2', 'struct<_1:bigint,_2:string>')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**show**()\n",
        "테이블 형태로 데이터를 출력하며 첫 20개의 열만 전시한다.\n",
        "\n",
        "디버깅할 때 유용하게 쓰인다."
      ],
      "metadata": {
        "id": "q98ttyxxyFkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVDppr6WyLnD",
        "outputId": "27182d72-4543-4545-ca56-1c371ccc6585"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+\n",
            "| _1|           _2|\n",
            "+---+-------------+\n",
            "|  1|  {2984, USD}|\n",
            "|  2|   {645, USD}|\n",
            "|  3|  {3518, USD}|\n",
            "|  4|  {1222, USD}|\n",
            "|  5| {70600, USD}|\n",
            "|  6|{125000, USD}|\n",
            "+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**printSchema**()\n",
        "스키마를 트리 형태로 볼 수 있다.\n",
        "중첩된 스키마라면 이 방법이 편하다."
      ],
      "metadata": {
        "id": "FzXY_D6oyO2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIDWgTsoyONL",
        "outputId": "fd5feee2-0d09-42f3-cc13-a92cf7f15dc1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _1: long (nullable = true)\n",
            " |-- _2: struct (nullable = true)\n",
            " |    |-- _1: long (nullable = true)\n",
            " |    |-- _2: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**복잡한 데이터 타입**\n",
        "\n",
        "ArrayType: 변수 타입\n",
        "\n",
        "MapType: 파이썬의 딕셔너리와 같은 형태\n",
        "\n",
        "StructType: 오브젝트\n",
        "\n",
        "**데이터 프레임의 작업**\n",
        "\n",
        "SQL과 비슷한 작업이 가능하다.\n",
        "\n",
        "Select\n",
        "\n",
        "Where\n",
        "\n",
        "Limit\n",
        "\n",
        "OrderBy\n",
        "\n",
        "GroupBy\n",
        "\n",
        "Join"
      ],
      "metadata": {
        "id": "mt5Pem03yFdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select**\n",
        "사용자가 원하는 컬럼이나 데이터를 추출하는데 사용한다."
      ],
      "metadata": {
        "id": "kSsnNzyTyY2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('*').collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-sFmaQsyZst",
        "outputId": "e228582e-99d9-46c6-8cd2-cb9fb5e2428a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_1=1, _2=Row(_1=2984, _2='USD')),\n",
              " Row(_1=2, _2=Row(_1=645, _2='USD')),\n",
              " Row(_1=3, _2=Row(_1=3518, _2='USD')),\n",
              " Row(_1=4, _2=Row(_1=1222, _2='USD')),\n",
              " Row(_1=5, _2=Row(_1=70600, _2='USD')),\n",
              " Row(_1=6, _2=Row(_1=125000, _2='USD'))]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agg**\n",
        "Aggregate의 약자로, 그룹핑 후 데이터를 하나로 합치는 작업이다."
      ],
      "metadata": {
        "id": "GXfJIosiykrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg({'price': 'max'}).collect()\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "df.agg(F.min(df.price)).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBBUbGriylqL",
        "outputId": "1dbb8529-daea-40d1-b617-e5622e517e89"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(min(price)=483)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GropBy**\n",
        "사용자가 지정한 컬럼을 기준으로 데이터를 그룹핑하는 작업이다."
      ],
      "metadata": {
        "id": "Ley2an1lywFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('currency').agg({'price': 'mean'}).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvmhEO8Nyyz0",
        "outputId": "57fca330-39a9-40dd-8bb2-1635307cebcc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(currency='KRW', avg(price)=97800.0),\n",
              " Row(currency='JPY', avg(price)=2006.0),\n",
              " Row(currency='HKD', avg(price)=483.0),\n",
              " Row(currency='USD', avg(price)=2092.25)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy([df.currency, df.price]).count().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLAIFr4Xyze1",
        "outputId": "98a983aa-fbc8-4c76-95da-741e7ff50b9a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(currency='USD', price=1222, count=1),\n",
              " Row(currency='USD', price=3518, count=1),\n",
              " Row(currency='HKD', price=483, count=1),\n",
              " Row(currency='USD', price=645, count=1),\n",
              " Row(currency='KRW', price=70600, count=1),\n",
              " Row(currency='JPY', price=2006, count=1),\n",
              " Row(currency='USD', price=2984, count=1),\n",
              " Row(currency='KRW', price=125000, count=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Join**\n",
        "다른 데이터 프레임과 사용자가 지정한 컬럼을 기준으로 합치는 작업이다.\n"
      ],
      "metadata": {
        "id": "Myt1nRGby3SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.join(earningsDF, 'name').select(df.name, earningsDF.eps).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W6V62Gjy7RD",
        "outputId": "a9e5697c-9494-4042-e9d0-eee0fb88bedd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Kakao', eps=705.0),\n",
              " Row(name='Samsung', eps=1780.0),\n",
              " Row(name='Tesla', eps=1.8600000143051147),\n",
              " Row(name='Google', eps=27.989999771118164),\n",
              " Row(name='Tencent', eps=11.010000228881836),\n",
              " Row(name='Toyota', eps=224.82000732421875),\n",
              " Row(name='Netflix', eps=2.559999942779541),\n",
              " Row(name='Amazon', eps=6.119999885559082)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "earnings = [\n",
        "    ('Google', 27.99, 'USD'), \n",
        "    ('Netflix', 2.56, 'USD'),\n",
        "    ('Amazon', 6.12, 'USD'),\n",
        "    ('Tesla', 1.86, 'USD'),\n",
        "    ('Tencent', 11.01, 'HKD'),\n",
        "    ('Toyota', 224.82, 'JPY'),\n",
        "    ('Samsung', 1780., 'KRW'),\n",
        "    ('Kakao', 705., 'KRW')\n",
        "]\n",
        "\n",
        "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
        "\n",
        "\n",
        "# 직접 스키마 타입 설정\n",
        "earningsSchema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"eps\", FloatType(), True),\n",
        "    StructField(\"currency\", StringType(), True),\n",
        "])\n",
        "\n",
        "\n",
        "# 데이터 프레임 생성\n",
        "earningsDF = spark.createDataFrame(data=earnings, schema=earningsSchema)\n",
        "\n",
        "\n",
        "earningsDF.dtypes\n",
        "\n",
        "\n",
        "earningsDF.createOrReplaceTempView(\"earnings\")"
      ],
      "metadata": {
        "id": "TCw1j3N1s67z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earningsDF.select(\"*\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfdSm_rLtCOT",
        "outputId": "ebf5415e-1487-4877-ce34-04595d7c24ca"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+--------+\n",
            "|   name|   eps|currency|\n",
            "+-------+------+--------+\n",
            "| Google| 27.99|     USD|\n",
            "|Netflix|  2.56|     USD|\n",
            "| Amazon|  6.12|     USD|\n",
            "|  Tesla|  1.86|     USD|\n",
            "|Tencent| 11.01|     HKD|\n",
            "| Toyota|224.82|     JPY|\n",
            "|Samsung|1780.0|     KRW|\n",
            "|  Kakao| 705.0|     KRW|\n",
            "+-------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM stocks JOIN earnings ON stocks.name = earnings.name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMUVNC5qtEID",
        "outputId": "e4e7167f-3228-4a87-aa10-3c9c0360387d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------+------+--------+-------+------+--------+\n",
            "|   name|ticker|  country| price|currency|   name|   eps|currency|\n",
            "+-------+------+---------+------+--------+-------+------+--------+\n",
            "|  Kakao|035720|    Korea|125000|     KRW|  Kakao| 705.0|     KRW|\n",
            "|Samsung|005930|    Korea| 70600|     KRW|Samsung|1780.0|     KRW|\n",
            "|  Tesla|  TSLA|      USA|  1222|     USD|  Tesla|  1.86|     USD|\n",
            "| Google| GOOGL|      USA|  2984|     USD| Google| 27.99|     USD|\n",
            "|Tencent|  0700|Hong Kong|   483|     HKD|Tencent| 11.01|     HKD|\n",
            "| Toyota|  7203|    Japan|  2006|     JPY| Toyota|224.82|     JPY|\n",
            "|Netflix|  NFLX|      USA|   645|     USD|Netflix|  2.56|     USD|\n",
            "| Amazon|  AMZN|      USA|  3518|     USD| Amazon|  6.12|     USD|\n",
            "+-------+------+---------+------+--------+-------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT stocks.name, (stocks.price / earnings.eps) FROM stocks JOIN earnings ON stocks.name = earnings.name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9GDw80PtHxj",
        "outputId": "cf22b51f-90b2-466c-ffa4-a0e72f7a4dd3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------------------------------+\n",
            "|   name|(CAST(price AS DOUBLE) / CAST(eps AS DOUBLE))|\n",
            "+-------+---------------------------------------------+\n",
            "|  Kakao|                            177.3049645390071|\n",
            "|Samsung|                           39.662921348314605|\n",
            "|  Tesla|                             656.989242258975|\n",
            "| Google|                            106.6095042658442|\n",
            "|Tencent|                            43.86920889728746|\n",
            "| Toyota|                            8.922693419839167|\n",
            "|Netflix|                            251.9531306315913|\n",
            "| Amazon|                            574.8366120563447|\n",
            "+-------+---------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}