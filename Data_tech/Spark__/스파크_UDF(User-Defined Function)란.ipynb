{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **스파크 colab에 환경 구현**"
      ],
      "metadata": {
        "id": "a_xbGA760V0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwEL73ilw7oe"
      },
      "outputs": [],
      "source": [
        "# jdk 툴 설치\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# apache spark download - 하둡 버전에 따라 spakr 와 하둡 변경 (밑에 tgz 파일 설치 및 pip install)\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "# 명령어로 spark - hadoop 파일 압축 풀기\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "# spark 설치\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# spark 환경변수 설정 - java(jdk) ,spark(spark) 2개다 설정해줘야함\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wuSEjqETxB5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import spark 말고 findspark로 spark파일을 못찾는경우에 빠르게 설치 가능\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "SCLyWWr2_yn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "id": "jfW9-pXT_08D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UDF(User-Defined Function)란?**"
      ],
      "metadata": {
        "id": "Nne7c_ytJsm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sql문 안에서 쓸 수 있는 하나의 함수를 정의하는 것"
      ],
      "metadata": {
        "id": "wDbeo0nkZeEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# 스파크 인스턴스 생성\n",
        "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
        "\n",
        "\n",
        "# 실습을 위한 데이터\n",
        "transactions = [\n",
        "    ('찹쌀탕수육+짜장2', '2021-11-07 13:20:00', 22000, 'KRW'),\n",
        "    ('등심탕수육+크립새우+짜장면', '2021-10-24 11:19:00', 21500, 'KRW'), \n",
        "    ('월남 쌈 2인 세트', '2021-07-25 11:12:40', 42000, 'KRW'), \n",
        "    ('콩국수+열무비빔국수', '2021-07-10 08:20:00', 21250, 'KRW'), \n",
        "    ('장어소금+고추장구이', '2021-07-01 05:36:00', 68700, 'KRW'), \n",
        "    ('족발', '2020-08-19 19:04:00', 32000, 'KRW'),  \n",
        "]\n",
        "\n",
        "schema = [\"name\", \"datetime\", \"price\", \"currency\"]\n",
        "\n",
        "df = spark.createDataFrame(data=transactions, schema=schema)"
      ],
      "metadata": {
        "id": "HSHIAR4BZdUS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"transactions\")"
      ],
      "metadata": {
        "id": "IOBElG5qZili"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM transactions\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VscHX7ADZj8y",
        "outputId": "eff038c9-5ffd-48eb-c46e-407494d33f26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+-------------------+-----+--------+\n",
            "|                      name|           datetime|price|currency|\n",
            "+--------------------------+-------------------+-----+--------+\n",
            "|          찹쌀탕수육+짜장2|2021-11-07 13:20:00|22000|     KRW|\n",
            "|등심탕수육+크립새우+짜장면|2021-10-24 11:19:00|21500|     KRW|\n",
            "|          월남 쌈 2인 세트|2021-07-25 11:12:40|42000|     KRW|\n",
            "|       콩국수+열무비빔국수|2021-07-10 08:20:00|21250|     KRW|\n",
            "|       장어소금+고추장구이|2021-07-01 05:36:00|68700|     KRW|\n",
            "|                      족발|2020-08-19 19:04:00|32000|     KRW|\n",
            "+--------------------------+-------------------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UDF\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import LongType # 리턴 타입이 문자열이 되지 않게하기 위한 라이브러리\n",
        "\n",
        "# @udf('long') # 좀 더 파이써닉한 방법이며, 레지스팅과 타입 지정까지 할 수 있다. 왜인지 이 방법은 안된다.\n",
        "def squared(n): # 제곱하는 함수 생성\n",
        "    return n * n\n"
      ],
      "metadata": {
        "id": "aWixRcz-Zl1x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataFrame = spark.table('transactions')"
      ],
      "metadata": {
        "id": "cLS8lOVsZqG1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.udf.register(\"squared\", squared, LongType()) # 함수를 레지스터 하는 과정이 필요하다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoLS0ltcZuHx",
        "outputId": "e2f46823-6f34-421d-801c-1b4f93064d01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.squared(n)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT price, squared(price) FROM transactions\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75Ul9IHUZvza",
        "outputId": "c8fb6bc8-58c5-4328-da04-f7634abb2733"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+\n",
            "|price|squared(price)|\n",
            "+-----+--------------+\n",
            "|22000|     484000000|\n",
            "|21500|     462250000|\n",
            "|42000|    1764000000|\n",
            "|21250|     451562500|\n",
            "|68700|    4719690000|\n",
            "|32000|    1024000000|\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자를 한글로 읽는 함수 생성\n",
        "def read_number(n):\n",
        "    units = [\"\", \"십\", \"백\", \"천\", \"만\"]\n",
        "    nums = '일이삼사오육칠팔구'\n",
        "    result = []\n",
        "    i = 0\n",
        "    while n > 0:\n",
        "        n, r = divmod(n, 10) # 나눈 결과와 나머지를 반환하는 함수\n",
        "        if r > 0:\n",
        "            result.append(nums[r-1]+units[i])\n",
        "        i += 1\n",
        "    return \"\".join(reversed(result))\n",
        "\n",
        "spark.udf.register(\"read_number\", read_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBPqsRB_Z2n6",
        "outputId": "409ce7a2-e2b2-4deb-ff2e-3c108da23906"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.read_number(n)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(read_number(21250))\n",
        "print(read_number(68700))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MITjQo1YZ4Dh",
        "outputId": "ab69e89f-629d-450e-867e-d48bfc2ad153"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이만일천이백오십\n",
            "육만팔천칠백\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT price, read_number(price) FROM transactions\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqmYfDAJZ7nC",
        "outputId": "8c4d9759-ca57-47a1-b8ab-1938952e1002"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+\n",
            "|price|read_number(price)|\n",
            "+-----+------------------+\n",
            "|22000|          이만이천|\n",
            "|21500|      이만일천오백|\n",
            "|42000|          사만이천|\n",
            "|21250|  이만일천이백오십|\n",
            "|68700|      육만팔천칠백|\n",
            "|32000|          삼만이천|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 요일을 반환하는 함수 생성\n",
        "def get_weekday(date):\n",
        "    import calendar\n",
        "    return calendar.day_name[date.weekday()]\n",
        "\n",
        "spark.udf.register(\"get_weekday\", get_weekday)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGAoTgmMZ9Sz",
        "outputId": "d14755ac-30e2-4273-b7ee-b413e6524917"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.get_weekday(date)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "    datetime,\n",
        "    get_weekday(TO_DATE(datetime)) as day_of_week\n",
        "FROM\n",
        "    transactions\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whZP9uqdZ_Oj",
        "outputId": "a76c5e44-d47f-467b-bb14-06ebbca22d9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+\n",
            "|           datetime|day_of_week|\n",
            "+-------------------+-----------+\n",
            "|2021-11-07 13:20:00|     Sunday|\n",
            "|2021-10-24 11:19:00|     Sunday|\n",
            "|2021-07-25 11:12:40|     Sunday|\n",
            "|2021-07-10 08:20:00|   Saturday|\n",
            "|2021-07-01 05:36:00|   Thursday|\n",
            "|2020-08-19 19:04:00|  Wednesday|\n",
            "+-------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}