{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark의 Session과 Context**"
      ],
      "metadata": {
        "id": "kRn63G5jpIRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스파크 컨텍스트**\n",
        "\n",
        "spark 2.0.0 이전에는 sparkContext가 모든 spark 기능에 액세스 하기 위한 채널로 사용됨. 스파크 드라이버 프로그램은 스파크 컨텍스트를 사용하여 리소스 관리자(yarn ...등)를 통해 클러스터에 연결.sparkConf는 appName(spark 드라이버 식별용), 애플리케이션, 작업자 노드에서 실행중인 실행기의 코어 수 및 메모리 크기와 같은 구성 매개 변수를 저장하는 spark 컨텍스트 개체를 생성하는데 필요\n",
        "\n",
        "SQL, HIVE, Streaming API를 사용하기 위해서는 별도의 CONTEXT를 생성해야한다."
      ],
      "metadata": {
        "id": "Ypn5k3tipL8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creation of SparkConf:\n",
        "conf = new SparkConf().setAppName(“RetailDataAnalysis”).setMaster(“spark://master:7077”).set(“spark.executor.memory”, “2g”)\n",
        "\n",
        "# creation of sparkContext:\n",
        "sc = new SparkContext(conf)\n"
      ],
      "metadata": {
        "id": "9ANHKO27qNca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**스파크세션**\n",
        "\n",
        "spark 2.0.0부터 sparksession은 기본 spark기능과 상호 작용할 수 있는 단일 진입점을 제공하며, DataFrame 및 set api로 spark를 프로그래밍 할 수 있다. sparkcontext에서 사용 할 수 있는 모든 기능은 sparksession에서도 사용할 수 있다.\n",
        "\n",
        "SQL, HIVE, Streaming API를 사용하기 위해 sparksession에는 모든 api가 포함되어 있으므로 별도의 컨텍스트를 생성할 필요 X, sparksession이 인스턴스화되면 Spark의 런타임 구성 속성을 구성할 수있다."
      ],
      "metadata": {
        "id": "dWa1-RkbpNJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Spark session:\n",
        "spark = SparkSession\n",
        "  .builder\n",
        "  .appName(\"WorldBankIndex\")\n",
        "  .getOrCreate()\n",
        "\n",
        "# Configuring properties:\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
        "spark.conf.set(\"spark.executor.memory\", \"2g\")"
      ],
      "metadata": {
        "id": "MzeBjOUFqNDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}